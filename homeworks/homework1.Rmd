---
title: "Homework_1 - Bayesian Statistics"
author: "Domagoj Korais"
date: "Spring 2019"
output: 
  html_document:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(cache = FALSE)
library(tidyverse)
library(arm)
set.seed(pi)
```

## TH-1

Let's start reproducing the scatter plot from slide 20.

A quick overview of the used dataset:

```{r th1}
summary(cars)
ggplot(cars,aes(x=speed,y=dist))+
geom_point()+
theme_light()+
ggtitle("Cars dataset dist vs speed")

```

 Non informative vs informative prior linear regression:
 
```{r fit full dataset}

nonInfromativeFit = bayesglm(dist ~ speed,data = cars, family = gaussian ,prior.mean = 1, prior.scale = Inf)



informativeFit=bayesglm(dist ~ speed,
                            data = cars, 
                            family = gaussian,
                            prior.mean = 0, 
                            prior.scale = 0.01,
                            prior.mean.for.intercept = 0,
                            prior.scale.for.intercept = 0.0001)
display(informativeFit)
display(nonInfromativeFit)



ggplot(cars,aes(x=speed,y=dist))+
geom_point()+
theme_light()+
ggtitle("Cars dataset dist vs speed")+
  
geom_abline(aes(intercept=informativeFit$coefficients[1],    slope=informativeFit$coefficients[2], col = "Informative"))+
  
geom_abline(aes(intercept=nonInfromativeFit$coefficients[1], slope=nonInfromativeFit$coefficients[2], col="Non informative"))+
  scale_color_manual(name='Priors', values=c("red", "green"))
```

The results from both the informative and non informative priors are very similar since it's a highly informative dataset.

Let's try with a subset of the dataset

```{r fit subset}
cars = sample_n(cars,15)
nonInfromativeFit = bayesglm(dist ~ speed,data = cars, family = gaussian ,prior.mean = 1, prior.scale = Inf)



informativeFit=bayesglm(dist ~ speed,
                            data = cars, 
                            family = gaussian,
                            prior.mean = 0, 
                            prior.scale = 0.01,
                            prior.mean.for.intercept = 0,
                            prior.scale.for.intercept = 0.0001)
display(informativeFit)
display(nonInfromativeFit)



ggplot(cars,aes(x=speed,y=dist))+
geom_point()+
theme_light()+
ggtitle("Cars dataset dist vs speed")+
  
geom_abline(aes(intercept=informativeFit$coefficients[1],    slope=informativeFit$coefficients[2], col = "Informative"))+
  
geom_abline(aes(intercept=nonInfromativeFit$coefficients[1], slope=nonInfromativeFit$coefficients[2], col="Non informative"))+
  scale_color_manual(name='Priors', values=c("red", "green"))
```

In this case we can see that the prior choice is important since it affects the final estimation of the parameters.

## BDA-1
2.7a)
Given the exponential form: $h(y)g(\theta)e^{(\eta(\theta)*T(y))}$ we can rewrite the binomial as:
${N \choose y} (1-\theta)^n exp(y *log(\frac{\theta}{1-\theta}))$ 
With $\eta(\theta)=log(\frac{\theta}{1-\theta}) $ as the natural parameter. 
Now we invert this quantity and we get $\theta=\frac{e^\phi}{1+e^\phi}$.
At this point we use the result from Gelman (pag 21) to obtain:
$p(\theta)\propto 1*\frac{d}{d\theta}log(\frac{\theta}{1-\theta})\propto\frac{1}{\theta(1-\theta)}$

2.8a)
The posterior is 
$$\pi(\theta|y)\propto p(y|\theta)\pi(\theta)$$
$$\propto \theta^{y-1}(1-\theta)^{(n-y-1)}$$ 
and this quantity has an infinite value for y=1 or y=n, so it's integral it's infinite and the distribution is improper.

## BDA-2

2.8a
The solution is:
$\theta |y \sim N( {\frac{\frac{1}{40^2}180+\frac{n}{20^2}*150}{\frac{1}{40^2} +\frac{n}{20^2}},\frac{1}{\frac{1}{40^2} +\frac{n}{20^2}}})$

2.8b
The solution is:
$\tilde y |y \sim N( {\frac{\frac{1}{40^2}180+\frac{n}{20^2}*150}{\frac{1}{40^2} +\frac{n}{20^2}},\frac{1}{\frac{1}{40^2} +\frac{n}{20^2}}+20^2})$

2.8c
The 95% intervals are [138,163] for $\theta$ and [110,192] for $\tilde y$

2.8d
The 95% intervals are [146,154] for $\theta$ and [111,189] for $\tilde y$

## LAB-1

In this dataset we have different observation times so we need to account for that.

```{r lab1}
library(AER)
data("ShipAccidents")
summary(ShipAccidents)

#clean data from unobserved elements
cleanData=ShipAccidents%>%
  filter(service>0)

#fit the data with the proper posterior (pag 45 Gelman)
posterior <- function(x,arrayPoisson,exposures,alpha,beta){
  alpha=alpha + sum(arrayPoisson)
  beta=beta+sum(exposures)
  print(alpha)
  print(beta)
  return (dgamma(x,alpha,beta))
}


#creo hyperparameters, quelli usati nella scorsa esercitazione vanno cambiati in quanto estremamente distanti dal nuovo modello.
h1 = c(5,1,10,1/2)#ultimo elemento Ã¨ per la Jeffrey prior  
h2 = c(10,2,20,0) 

#show the priors: 
xmax=1
curve(dgamma(x,h1[1],h2[1]), from=0, to=xmax,ylim=c(0,3) , xlab="x", ylab="y",n=10000,main="Prior distributions")
curve(dgamma(x,h1[2],h2[2]), from=0, to=xmax, xlab="x", ylab="y", add = TRUE,col = "red",n=10000)
curve(dgamma(x,h1[3],h2[3]), from=0, to=xmax, xlab="x", ylab="y", add = TRUE,col = "green",n=10000)
curve(x**(-1/2), from=0, to=xmax, xlab="x", ylab="y",add = TRUE,col = "yellow",n=10000)
legend(0.8, 3,
       legend=c("Gamma1", "Gamma2","Gamma3","Jeffreys'"),
       col=c(1:4), lty=1, cex=0.8)

incidents=cleanData$incidents
incidentsCount=count(cleanData)$n
for (i in 1:length(h1))
{
  if (i ==1){flag=FALSE}
  else {flag=TRUE}
  
  curve(posterior(x,cleanData$incidents,exposures = cleanData$service,h1[i],h2[i]),main="Montly posterior",col=i,add=flag, from=0, to=0.005,ylim=c(0,200) , xlab="x", ylab="y",n=10000)
}
curve(posterior(x,cleanData$incidents,exposures = cleanData$service,h1[i],h2[i]),main="Montly posterior",col=i,add=flag, from=0, to=0.005,ylim=c(0,200) , xlab="x", ylab="y",n=10000)
legend(0.004, 200,
       legend=c("Gamma1", "Gamma2","Gamma3","Jeffreys'"),
       col=c(1:5), lty=1, cex=0.8)


```


3) Find the posterior expectations and the maxima a posteriori (MAP) 

```{r MAP and posterior}
## posterior expectations
(h1+sum(incidents))/(h2+sum(cleanData$service))
##MAP
(h1+sum(incidents)-1)/(h2+sum(cleanData$service))
```

4) Constructing the 95% credibility intervals with Jeffrey prior

```{r}
alpha=0.05
qgamma(c(alpha/2,1-alpha/2),sum(incidents) + 1/2 ,sum(cleanData$service))
```

5) Computing mode and the highest posterior density range
```{r}
hpd<-function(y,p){
dy<-density(y)
md<-dy$x[dy$y==max(dy$y)]
py<-dy$y/sum(dy$y)
pys<--sort(-py)
ct<-min(pys[cumsum(pys)< p])
list(hpdr=range(dy$x[py>=ct]),mode=md)
}

hpd(rgamma(10000,.5+sum(incidents),sum(cleanData$service)),.95)
```




